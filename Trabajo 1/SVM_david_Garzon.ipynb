{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Nombre\n",
        "David Antonio Garzón Avendaño \n",
        "# Code Assigment 1\n",
        "\n",
        "For this assignment you will use the following SVM implementation for classifying these datasets:\n",
        "\n",
        "https://archive.ics.uci.edu/ml/datasets/banknote+authentication\n",
        "\n",
        "\n",
        "https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+\n",
        "\n",
        "You should:\n",
        "\n",
        "1) Specify which Machine Learning problem are you solving.\n",
        "\n",
        "2) Provide a short summary of the features and the labels you are working on.\n",
        "\n",
        "3) Please answer the following questions: a) Are these datasets linearly separable? b) Are these datasets randomly chosen and c) The sample size is enough to guarantee generalization.\n",
        "\n",
        "4) Provide an explanation how and why the code is working. You can add comments and/or formal explanations into the notebook.\n",
        "\n",
        "5) Show some examples to illustrate that the method is working properly.\n",
        "\n",
        "6) Provide quantitative evidence for generalization using the provided dataset.\n"
      ],
      "metadata": {
        "id": "s-y8Kil2snGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Respuestas\n"
      ],
      "metadata": {
        "id": "KYYI5x4na0t5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Punto 1)\n"
      ],
      "metadata": {
        "id": "TyqKAsk6a39L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###Dataset Uno. \n",
        "Deseamos que dadas ciertas caracteristicas de una imagen de un billete, crear un modelo que sea capaz de identificar si un billete es falso o no. Para ello tenemos como datos:\n",
        "* La varianza de la imagen transformada de wavelet\n",
        "* La asimetría de la imagen transformada por wavelet\n",
        "* La curtosis de la imagen transformada de wavelet\n",
        "* La entropía de la imagen.\n",
        "\n",
        "###Dataset Dos\n",
        "\n",
        "Deseamos que dadas unas caracteristicas de un billete, crear un modelo que sea capaz de identificar identifica si hay personas o no dentro de un cuarto, para ello tenemos. Para ello tenemos como datos:\n",
        "* Fecha y hora en que se tomó la medición\n",
        "* Temperatura en grados Celsius\n",
        "* Humedad relativa en porcentaje (%)\n",
        "* Luz en lux\n",
        "* CO2 en partes por millón (ppm)\n",
        "* Razón de humedad en kg de vapor de agua por kg de aire"
      ],
      "metadata": {
        "id": "ZgzeJS2Xb2j5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Punto 2) \n"
      ],
      "metadata": {
        "id": "8-0as5pIbB4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dataset Uno. \n",
        "\n",
        "Los atributos describen las siguientes características de las imágenes transformadas por wavelet:\n",
        "\n",
        "  * La varianza de la imagen transformada de wavelet. Es una medida de cuánto se extienden los valores de la imagen en el espacio de la transformada de wavelet.\n",
        "\n",
        "  * La asimetría de la imagen transformada de wavelet. Es una medida de la falta de simetría en la distribución de los valores de la imagen en el espacio de la transformada de wavelet.\n",
        "\n",
        "  * La curtosis de la imagen transformada de wavelet. Es una medida de la forma de la distribución de los valores de la imagen en el espacio de la transformada de wavelet.\n",
        "\n",
        "  * La entropía de la imagen. Es una medida de la incertidumbre o desorden en la distribución de los valores de los píxeles en la imagen.\n",
        "\n",
        "Los datos tienen las siguientes etiquetas:\n",
        "\n",
        "  * La clase de la imagen, que es un valor entero que indica si la imagen es auténtica (clase 0) o falsa (clase 1).\n",
        "\n",
        "###Dataset Dos. \n",
        "\n",
        "Los atributos describen las siguientes características:\n",
        "\n",
        "* Fecha y hora en que se tomó la medición. Proporciona información temporal sobre las mediciones.\n",
        "\n",
        "* Temperatura en grados Celsius. La temperatura es una medida de la cantidad de calor en el ambiente y afecta el nivel de comodidad de las personas.\n",
        "\n",
        "* Humedad relativa en porcentaje (%). La humedad relativa es una medida de la cantidad de vapor de agua en el ambiente y afecta el nivel de comodidad de las personas.\n",
        "\n",
        "* Luz en lux. La luz se mide en lux y representa la cantidad de luz visible en el ambiente.\n",
        "\n",
        "* CO2 en partes por millón (ppm). El CO2 se mide en ppm y es una medida de la calidad del aire interior.\n",
        "\n",
        "* Razón de humedad en kg de vapor de agua por kg de aire. La razón de humedad es una medida de la cantidad de vapor de agua en el aire y se utiliza para calcular otras medidas de humedad.\n",
        "\n",
        "  Los datos tiene las siguientes etiquetas:\n",
        "\n",
        "* Ocupación. Indica si el ambiente está ocupado o no. Esta variable es importante para estudiar la relación entre la ocupación y las medidas del ambiente. "
      ],
      "metadata": {
        "id": "kaT0UAatbzct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Procesamiento de los datos"
      ],
      "metadata": {
        "id": "7sb103N3gZ6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antes de responder el resto de las preguntas, vamos a organizar y cargar los datasets en dataframes para que sea más facil trabajar los datos."
      ],
      "metadata": {
        "id": "lRwSkVvIgh7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "FGqxMzeXg6EE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset 1"
      ],
      "metadata": {
        "id": "1GPkMM9zhCPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos y organizamos el dataset 1\n",
        "data_bank = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt', header=None)\n",
        "data_bank.columns = [\"variance\", \"skewness\", \"curtosis\", \"entropy\", \"class\"]\n",
        "\n",
        "# Remplazamos los 0 por -1 para la SVM\n",
        "data_bank[\"class\"] = data_bank[\"class\"].replace([0],-1)\n",
        "\n",
        "# Separamos los atributos de las etiquetas\n",
        "# Atributos \n",
        "X_data_bank = data_bank[[\"variance\", \"skewness\", \"curtosis\", \"entropy\"]]\n",
        "#Etiquetas\n",
        "y_data_bank = data_bank[\"class\"]"
      ],
      "metadata": {
        "id": "o8zY6ODOhHCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_data_bank.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "BcIyXnIRiHLq",
        "outputId": "d3487438-a36e-428c-e261-b0ede5af714e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   variance  skewness  curtosis  entropy\n",
              "0   3.62160    8.6661   -2.8073 -0.44699\n",
              "1   4.54590    8.1674   -2.4586 -1.46210\n",
              "2   3.86600   -2.6383    1.9242  0.10645\n",
              "3   3.45660    9.5228   -4.0112 -3.59440\n",
              "4   0.32924   -4.4552    4.5718 -0.98880"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c696a94c-3877-43a3-bde7-caba7cbcb371\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>variance</th>\n",
              "      <th>skewness</th>\n",
              "      <th>curtosis</th>\n",
              "      <th>entropy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.62160</td>\n",
              "      <td>8.6661</td>\n",
              "      <td>-2.8073</td>\n",
              "      <td>-0.44699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.54590</td>\n",
              "      <td>8.1674</td>\n",
              "      <td>-2.4586</td>\n",
              "      <td>-1.46210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.86600</td>\n",
              "      <td>-2.6383</td>\n",
              "      <td>1.9242</td>\n",
              "      <td>0.10645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.45660</td>\n",
              "      <td>9.5228</td>\n",
              "      <td>-4.0112</td>\n",
              "      <td>-3.59440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.32924</td>\n",
              "      <td>-4.4552</td>\n",
              "      <td>4.5718</td>\n",
              "      <td>-0.98880</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c696a94c-3877-43a3-bde7-caba7cbcb371')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c696a94c-3877-43a3-bde7-caba7cbcb371 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c696a94c-3877-43a3-bde7-caba7cbcb371');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_data_bank.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7G3xtPTiJhQ",
        "outputId": "3c452180-b8c0-4f5a-b33d-812451b4b87e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0   -1\n",
              "1   -1\n",
              "2   -1\n",
              "3   -1\n",
              "4   -1\n",
              "Name: class, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset 2"
      ],
      "metadata": {
        "id": "V3FgDW2Pidwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#En este caso como el archivo es un Zip hay que hacer un par adicional de pasos\n",
        "import urllib\n",
        "import zipfile\n",
        "\n",
        "# Descargamos y descomprimos los datos\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00357/occupancy_data.zip\"\n",
        "extract_dir = \"occupancy\"\n",
        "\n",
        "zip_path, _ = urllib.request.urlretrieve(url)\n",
        "with zipfile.ZipFile(zip_path, \"r\") as f:\n",
        "    f.extractall(extract_dir)"
      ],
      "metadata": {
        "id": "zR1F6FnqikKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Como los datos están separados de una vez para\n",
        "# entrenar y hacer testing del modelo, es necesario \n",
        "# crear varios dataframes\n",
        "\n",
        "def occupancy_df_process(path):\n",
        "  data = pd.read_csv(path, sep=\",\", \n",
        "            names=[\"col\",\"date\",\"Temperature\",\"Humidity\",\n",
        "                   \"Light\",\"CO2\",\"HumidityRatio\",\"Occupancy\"])[1:]\n",
        "\n",
        "  data = data.drop([\"col\"],axis=1)\n",
        "  #ya no usamos date, pero pues el código está lindo \n",
        "  data['date'] = data['date'].apply(lambda x: pd.to_datetime(x).to_julian_date())\n",
        "  data[\"Light\"] = data[\"Light\"].astype(float)\n",
        "  data[\"Temperature\"] = data[\"Temperature\"].astype(float)\n",
        "  data[\"Humidity\"] = data[\"Humidity\"].astype(float)\n",
        "  data[\"CO2\"] = data[\"CO2\"].astype(float)\n",
        "  data[\"HumidityRatio\"] = data[\"HumidityRatio\"].astype(float)\n",
        "  data[\"Occupancy\"] = data[\"Occupancy\"].astype(float)\n",
        "\n",
        "\n",
        "  # Remplazamos los 0 por -1 para la SVM\n",
        "  data[\"Occupancy\"] = data[\"Occupancy\"].replace([0],-1)\n",
        "\n",
        "  # Etiquetas\n",
        "  X_data = data[[\"Temperature\",\"Humidity\",\n",
        "                   \"Light\",\"CO2\",\"HumidityRatio\"]]\n",
        "  # Atributos\n",
        "  Y_data = data[\"Occupancy\"]\n",
        "  return X_data, Y_data\n",
        "\n",
        "occupancy_train_X , occupancy_train_y = occupancy_df_process(\"occupancy/datatraining.txt\")\n",
        "occupancy_test_X , occupancy_test_y = occupancy_df_process(\"occupancy/datatest.txt\")\n",
        "occupancy_test2_X , occupancy_test2_y = occupancy_df_process(\"occupancy/datatest2.txt\")"
      ],
      "metadata": {
        "id": "ExrhvY7tj-kF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "occupancy_train_X.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "em3uZ2O72U17",
        "outputId": "ef42d7ca-1096-43f4-8a30-01550095f47e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Temperature      float64\n",
              "Humidity         float64\n",
              "Light            float64\n",
              "CO2              float64\n",
              "HumidityRatio    float64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(occupancy_train_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjhrHJIL3IRU",
        "outputId": "c869db62-6dee-4561-bcf3-191b9d99789b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1.0, -1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Punto 3)"
      ],
      "metadata": {
        "id": "qUceMwccb6K7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a) \n",
        "Para determinar si los dataset son separables, vamos a usar el algoritmo Percetron, ya que si este algoritmo converge en una cantidad finita de pasos, entonces nuestro dataset es separable. \n"
      ],
      "metadata": {
        "id": "cQ86gs_1b-0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "VlcjWPSnK2Wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perceptron_test(X_train, y_train):\n",
        "  # Crear una instancia del clasificador Perceptron\n",
        "  clf = Perceptron()\n",
        "\n",
        "  # Entrenar el modelo con el conjunto de datos de entrenamiento\n",
        "  clf.fit(X_train, y_train)\n",
        "  score = clf.score(X_train, y_train)\n",
        "  print(score)\n",
        "\n",
        "  # Verificar si el conjunto de datos es linealmente separable\n",
        "  if clf.score(X_train, y_train) <= 9.8:\n",
        "      print(\"El conjunto de datos es linealmente separable.\")\n",
        "      return True\n",
        "  else:\n",
        "      print(\"El conjunto de datos no es linealmente separable.\")\n",
        "      return False \n"
      ],
      "metadata": {
        "id": "yjOTAA7Bdm5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset 1"
      ],
      "metadata": {
        "id": "u6af1hyIeSZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test del percetron \n",
        "perceptron_test(X_data_bank.to_numpy(),y_data_bank.to_numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XItAmzGDeaZq",
        "outputId": "24d9dade-cd1f-4dd2-82b1-05db9a356b25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9883381924198251\n",
            "El conjunto de datos es linealmente separable.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset 2"
      ],
      "metadata": {
        "id": "JV8X55WTeXrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test del percetron \n",
        "perceptron_test(occupancy_train_X, occupancy_train_y )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhkF-9gyi4tM",
        "outputId": "47effcc8-7f6a-4ae3-f1d9-6da019e43241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9815792705391133\n",
            "El conjunto de datos es linealmente separable.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este caso podemos evidenciar que ninguno de los dos dataset son linealmente separables si seguimos estrictamente la definición, ya que no pudieron ser separados mediante el algoritmo Perceptron, sin embargo recordemos que los datos son muestras del mundo real, por lo que es de esperarse algún tipo de error en las mediciones. Por lo tanto al tener una cantidad tan baja de elementos que no podemos separar (menos del 2%) podemos considerar que ambos datasets son linealmete separables. "
      ],
      "metadata": {
        "id": "Zh5Un0Mm5mrl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b) \n",
        "No es posible determinar si los datos del dataset son tomados de manera aleatoria, sin embargo confiamos y partimos de eses hecho para poder hacer el resto del análsis y crear modelos entrenados con esos datos "
      ],
      "metadata": {
        "id": "SJaa-59H6-A-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c) \n",
        "Como regla general, se recomienda que la muestra de datos tenga al menos cientos o miles de observaciones, dependiendo de la cantidad de características y el grado de complejidad del problema. Como nuestros datasets tienen miles de datos, podemos pensar que dada esta cantidad de información es posible que algún modelo pueda llegar a una generalización. "
      ],
      "metadata": {
        "id": "BzcQ2QfX7P7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Punto 4) "
      ],
      "metadata": {
        "id": "yIWm-bD4uJtY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para entender cómo opera el siguiente código, vamos describir brevemente dentro del código que hace cada una de sus partes mediante el uso de los comentarios de python. "
      ],
      "metadata": {
        "id": "sk-JX-vy4nLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SVM:\n",
        "    \n",
        "    def __init__(self, C = 1.0):\n",
        "        # C es el parámetro de error\n",
        "        self.C = C\n",
        "        self.w = 0\n",
        "        self.b = 0\n",
        "\n",
        "    # Función de Pérdida Hinge / Cálculo\n",
        "    def hingeloss(self, w, b, x, y):\n",
        "        # Término de Regularización\n",
        "        reg = 0.5 * (w * w)\n",
        "\n",
        "        for i in range(x.shape[0]):\n",
        "            # Término de Optimización\n",
        "            opt_term = y[i] * ((np.dot(w, x[i])) + b)\n",
        "\n",
        "            # Calculando la pérdida\n",
        "            loss = reg + self.C * max(0, 1-opt_term)\n",
        "        return loss[0][0]\n",
        "\n",
        "    def fit(self, X, Y, batch_size=100, learning_rate=0.001, epochs=1000):\n",
        "        # Número de características en X\n",
        "        number_of_features = X.shape[1]\n",
        "\n",
        "        # Número de muestras en X\n",
        "        number_of_samples = X.shape[0]\n",
        "\n",
        "        c = self.C\n",
        "\n",
        "        # Creando ids desde 0 hasta número de muestras - 1\n",
        "        ids = np.arange(number_of_samples)\n",
        "\n",
        "        # Mezclando las muestras al azar\n",
        "        np.random.shuffle(ids)\n",
        "\n",
        "        # Creando un arreglo de ceros\n",
        "        w = np.zeros((1, number_of_features))\n",
        "        b = 0\n",
        "        losses = []\n",
        "\n",
        "        # Lógica del Descenso de Gradiente\n",
        "        for i in range(epochs):\n",
        "            # Calculando la Función de Pérdida Hinge\n",
        "            l = self.hingeloss(w, b, X, Y)\n",
        "\n",
        "            # Añadiendo todas las pérdidas\n",
        "            losses.append(l)\n",
        "            \n",
        "            # Iniciando desde 0 hasta el número de muestras con batch_size como intervalo\n",
        "            for batch_initial in range(0, number_of_samples, batch_size):\n",
        "                gradw = 0\n",
        "                gradb = 0\n",
        "\n",
        "                for j in range(batch_initial, batch_initial+ batch_size):\n",
        "                    if j < number_of_samples:\n",
        "                        x = ids[j]\n",
        "                        ti = Y[x] * (np.dot(w, X[x].T) + b)\n",
        "\n",
        "                        if ti > 1:\n",
        "                            gradw += 0\n",
        "                            gradb += 0\n",
        "                        else:\n",
        "                            # Calculando los gradientes\n",
        "\n",
        "                            # respectivos a w \n",
        "                            gradw += c * Y[x] * X[x]\n",
        "                            # respectivos a b\n",
        "                            gradb += c * Y[x]\n",
        "\n",
        "                # Actualizando pesos y bias\n",
        "                w = w - learning_rate * w + learning_rate * gradw\n",
        "                b = b + learning_rate * gradb\n",
        "        \n",
        "        self.w = w\n",
        "        self.b = b\n",
        "\n",
        "        return self.w, self.b, losses\n",
        "\n",
        "    def predict(self, X):\n",
        "        \n",
        "        prediction = np.dot(X, self.w[0]) + self.b # w.x + b\n",
        "        return np.sign(prediction)"
      ],
      "metadata": {
        "id": "--fvzOf_uQRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Punto 5)"
      ],
      "metadata": {
        "id": "vw5BQTvbxFvX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La clave del éxito de SVM radica en su capacidad para encontrar un hiperplano óptimo que tenga la mayor distancia posible entre los datos de diferentes clases. Esta distancia se conoce como margen y se mide perpendicularmente al hiperplano. El hiperplano óptimo se encuentra mediante la maximización de este margen."
      ],
      "metadata": {
        "id": "k6V30-EBsO2E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset 1"
      ],
      "metadata": {
        "id": "IqUafL10zBzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos la SVM \n",
        "bank_SVM = SVM()\n",
        "#Partimos los datos usando sklearn \n",
        "X_train, X_test,y_train, y_test = train_test_split(X_data_bank, y_data_bank ,\n",
        "                                   random_state=104, \n",
        "                                   test_size=0.25, \n",
        "                                   shuffle=True)\n",
        "#Entrenamos la SVM\n",
        "bank_SVM.fit(X_train.to_numpy(), y_train.to_numpy()); \n",
        "\n",
        "#Ahora evaluamos el rendimiento\n",
        "predicted_data = bank_SVM.predict(X_test.to_numpy())\n",
        "\n",
        "errors = len([i for i in (predicted_data - y_test) if i != 0]) \n",
        "\n",
        "score = (len(y_test) - errors)/len(y_test)\n",
        "print(score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSMFMAFKvs5g",
        "outputId": "ee23c60a-2d06-4058-f1d7-432a5aa16e88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9825072886297376\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note que este el codigó logra clasificar correctamente el 98% de datos en el primer dataset, pese a que este dataset no es linealmente separable. "
      ],
      "metadata": {
        "id": "uYyX15r6sSvo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset 2"
      ],
      "metadata": {
        "id": "p2RiliGj0SlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset a usar\n",
        "#occupancy_train_X , occupancy_train_y \n",
        "#occupancy_test_X , occupancy_test_y \n",
        "# occupancy_test2_X , occupancy_test2_y\n",
        "\n",
        "\n",
        "# Creamos la SVM \n",
        "Occupancy_SVM = SVM()\n",
        "\n",
        "# Los datos ya están partidos en secciones de prueba\n",
        "# y test\n",
        "\n",
        "#Entrenamos la SVM\n",
        "Occupancy_SVM.fit(occupancy_train_X.to_numpy(), occupancy_train_y.to_numpy())\n",
        "\n",
        "#Ahora evaluamos el rendimiento\n",
        "predicted_data = Occupancy_SVM.predict(occupancy_test_X.to_numpy())\n",
        "\n",
        "errors = len([i for i in (predicted_data - occupancy_test_y) if i != 0]) \n",
        "score = (len(occupancy_test_y) - errors)/len(occupancy_test_y)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nr_tg4EI0Rfx",
        "outputId": "4bb5f6c6-05c7-4850-bfe3-d74f9b36eb91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9711069418386492\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Punto 6)"
      ],
      "metadata": {
        "id": "OvD33hR_tE8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para este punto vamos realizar cross validation que es una técnica utilizada en el aprendizaje automático para evaluar el rendimiento de un modelo de predicción. La idea principal es dividir los datos en conjuntos de entrenamiento y prueba, donde el conjunto de entrenamiento se utiliza para entrenar el modelo y el conjunto de prueba se utiliza para evaluar su capacidad para generalizar a nuevos datos."
      ],
      "metadata": {
        "id": "IFrJky_suZVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset 1\n"
      ],
      "metadata": {
        "id": "BlDNtK_ouwDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "perf = []\n",
        "for _ in range(0,10):\n",
        "  # Creamos la SVM \n",
        "  bank_SVM = SVM()\n",
        "  #Partimos los datos usando sklearn \n",
        "  X_train, X_test,y_train, y_test = train_test_split(X_data_bank, y_data_bank ,\n",
        "                                    random_state=104, \n",
        "                                    test_size=0.25, \n",
        "                                    shuffle=True)\n",
        "  #Entrenamos la SVM\n",
        "  bank_SVM.fit(X_train.to_numpy(), y_train.to_numpy()); \n",
        "\n",
        "  #Ahora evaluamos el rendimiento\n",
        "  predicted_data = bank_SVM.predict(X_test.to_numpy())\n",
        "\n",
        "  errors = len([i for i in (predicted_data - y_test) if i != 0]) \n",
        "\n",
        "  score = (len(y_test) - errors)/len(y_test)\n",
        "  perf.append(score)"
      ],
      "metadata": {
        "id": "wsFXgsG0tEhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_score = sum(perf)/len(perf)\n",
        "print(f\"Resultado del cross validation {total_score}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULx68qOMwc14",
        "outputId": "603184d8-47c2-4f0f-f062-2311a97e4ac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado del cross validation 0.985131195335277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset 2"
      ],
      "metadata": {
        "id": "Un8ZHSGbuwO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como los datos viene previamente en separados para el training y el testing, para sacar diferentes subconjutos vamos a pegar ambos dataframes y luego usaremos una libreria para gener los subconjuntos."
      ],
      "metadata": {
        "id": "yBggi0MDSx7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset a usar\n",
        "#occupancy_train_X , occupancy_train_y \n",
        "#occupancy_test_X , occupancy_test_y \n",
        "\n",
        "#Nuevos datos\n",
        "ocuppancy_X = occupancy_train_X.append(occupancy_test_X)\n",
        "occupancy_y = occupancy_train_y.append(occupancy_test_y)"
      ],
      "metadata": {
        "id": "onC2eWpxSwKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(0,10):\n",
        "  # Separamos los datos\n",
        "  X_train, X_test,y_train, y_test = train_test_split(ocuppancy_X, occupancy_y,\n",
        "                                    random_state=104, \n",
        "                                    test_size=0.25, \n",
        "                                    shuffle=True)\n",
        "\n",
        "  # Creamos la SVM \n",
        "  Occupancy_SVM = SVM()\n",
        "  #Entrenamos la SVM\n",
        "  Occupancy_SVM.fit(X_train.to_numpy(), y_train.to_numpy())\n",
        "\n",
        "  #Ahora evaluamos el rendimiento\n",
        "  predicted_data = Occupancy_SVM.predict(X_test.to_numpy())\n",
        "\n",
        "  errors = len([i for i in (predicted_data -  y_test) if i != 0]) \n",
        "  score = (len(y_test) - errors)/len(y_test)\n",
        "  perf.append(score)"
      ],
      "metadata": {
        "id": "OfwqU_ofIQys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_score = sum(perf)/len(perf)\n",
        "print(f\"Resultado del cross validation {total_score}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksTEp82WVopR",
        "outputId": "67cc2bd0-470c-40be-b9af-dd7153410e9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado del cross validation 0.8706669751959148%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con lo anterior evidenciamos que nuestro algoritmo crea generalización, ya que independientemete de los datos que se usen para training y test, el promedio de aciertos está por encima del 98% para el primer modelo, y de 87% para el segúndo dataset. "
      ],
      "metadata": {
        "id": "cakJf8yAXuLq"
      }
    }
  ]
}